import numpy as np
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, PolynomialFeatures, StandardScaler
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import OrdinalEncoder

# Read the CSV file into a DataFrame
file_path = r'C:\Users\nimae\OneDrive\Desktop\Documents\Python\my_data.csv'
data = pd.read_csv(file_path)

# Extract data1 based on condition
data1 = data[20:75]  # Extract elements at index 22 to 74 inclusive

# Extract data2 based on condition
data2 = data[93:104]  # Extract elements at index 94 to 103 inclusive

# Reset index for data1 and data2
data1 = data1.reset_index(drop=True)
data2 = data2.reset_index(drop=True)

# Concatenate data1 and data2
data3 = pd.concat([data1, data2], ignore_index=True)

# Define the features (X) and labels (y)
X = pd.DataFrame(data3, columns=['SquareMeter', 'District'])
y = data3['Price'].values.reshape(-1, 1)

# Encode the categorical feature 'District' using ordinal encoding
encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)
X_encoded = encoder.fit_transform(X[['District']])

# Combine SquareMeter and encoded District features
X_combined = np.hstack((X[['SquareMeter']].values, X_encoded))

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

# Impute missing values with the mean of the column
imputer = SimpleImputer(strategy='mean')
imputer.fit(X_train)
X_train_imputed = imputer.transform(X_train)
X_test_imputed = imputer.transform(X_test)

# Create polynomial features
poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train_imputed)
X_test_poly = poly.transform(X_test_imputed)

# Feature Scaling (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_poly)
X_test_scaled = scaler.transform(X_test_poly)

# Initialize and train Polynomial Regression model
poly_reg = LinearRegression()
poly_reg.fit(X_train_scaled, y_train)

# Evaluate the Polynomial Regression model
result = poly_reg.score(X_test_scaled, y_test)
print("Polynomial Regression R^2 score:", result)

# Make predictions using Polynomial Regression
y_pred_poly = poly_reg.predict(X_test_scaled)

# Calculate Mean Squared Error for Polynomial Regression
mse_poly = mean_squared_error(y_test, y_pred_poly)
print(f"Polynomial Regression Mean Squared Error: {mse_poly:.2f}")

# Initialize and train Random Forest Regressor model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Evaluate the Random Forest Regressor model
result_rf = rf_model.score(X_test, y_test)
print("Random Forest Regressor R^2 score:", result_rf)

# Make predictions using Random Forest Regressor
y_pred_rf = rf_model.predict(X_test)

# Calculate Mean Squared Error for Random Forest Regressor
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f"Random Forest Regressor Mean Squared Error: {mse_rf:.2f}")

# Plot the predictions
plt.scatter(y_test, y_pred_poly, label='Polynomial Regression', color='blue')
plt.scatter(y_test, y_pred_rf, label='Random Forest Regressor', color='red')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Comparison of Predictions')
plt.legend()
plt.grid()
plt.show()
